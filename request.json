{
  "model": "llama-3.3-70b-versatile",
  "messages": [{ "role": "user", "content": "hello" }],
  "temperature": 0.7,
  "max_tokens": 100
}
